# -*- coding: utf-8 -*-
"""RekomendasiProyek.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Wm4tLgqD08umhisckaCxtQ-L_171G_aC

# **Proyek Kedua**

Sistem Rekomendasi Buku

Nama: Zulfazazalia Putri Candra Wati

Email: lia.putricandrawati@gmail.com

Domisili: Bali

Install opendataset yang nantinya digunakan untuk mengambil dataset
"""

!pip install opendatasets --upgrade

"""
Impor opendataset yang digunakan untuk mengambil dataset yang ada pada kaggel dimana nantinya memasukkan username dan key"""

# Kunci Kaggle
# "username":"zulfaliaputri"
# "key":"202a7841f910c6335d48aae386e35185"

import opendatasets as od

dataset_url = 'https://www.kaggle.com/arashnic/book-recommendation-dataset'
od.download('https://www.kaggle.com/arashnic/book-recommendation-dataset')

"""Library yang digunakan dalam proyek ini sebagai berikut:"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns
from sklearn.model_selection import train_test_split
from zipfile import ZipFile
from pathlib import Path
import matplotlib.pyplot as plt

"""# **Data Understanding**

Deskripsi dari variable pada dataset:

> Books.csv
*   ISBN: merupakan id pada buku dengan tipe data object
* Book-Title: merupakan judul dari buku dengan tipe data object
*   Book-Author: merupakan pengarang dari buku dengan tipe data object
* Year-Of-Publication: merupakan tahun di publikasinya buku dengan tipe data object
* Publisher: merupakan penerbit dari buku dengan tipe data object
* Image-URL-S: merupakan sampul gambar dari buku yang berukuran kecil dengan tipe data object
* Image-URL-M: merupakan sampul gambar dari buku yang berukuran sedang dengan tipe data object
* Image-URL-L: merupakan sampul gambar dari buku yang berukuran besar dengan tipe data object



> Rating.csv
* User-ID: merupakan ID dari user dengan tipe data int64
* ISBN: merupakan id pada buku dengan tipe data object
* Book-Rating: merupakan rating yang diberikan untuk buku dengan tipe data int64

Load dataset
"""

import pandas as pd
books = pd.read_csv('/content/book-recommendation-dataset/Books.csv')
rating = pd.read_csv('/content/book-recommendation-dataset/Ratings.csv')

"""## Univariate Exploratory Data Analysis

Menampilkan data yang ada pada dataset books setelah dilakukan load dataset
"""

books

"""Menampilkan info dari dataset 'books' yang bernilai object"""

books.info()

len(books)

"""Mengambil data buku yang digunakan sebanyak 12000 dari data yang berjumlah 271360 dengan menggunakan fungsi *iloc*"""

data_books = books.iloc[:12000]
len(data_books)

"""Menampilkan data yang ada pada dataset rating setelah dilakukan load dataset"""

rating

"""Menampilkan info dari dataset 'rating' yang bernilai integer dan object"""

rating.info()

len(rating)

"""Mengambil data rating yang digunakan sebanyak 12000 dari data yang berjumlah 1149780 dengan menggunakan fungsi *iloc*"""

data_rating = rating.iloc[:12000]
len(data_rating)

"""Melakukan *rename* kolom pada data_books yang digunakan untuk memudahkan menuliskan kolom"""

data_books.rename(columns = {'Book-Title':'Book_Title', 'Book-Author': 'Book_Author', 'Year-Of-Publication': 'Year_Publication'},inplace = True)
data_books

"""Melakukan *rename* kolom pada data_rating yang digunakan untuk memudahkan menuliskan kolom"""

data_rating.rename(columns = {'User-ID':'UserID', 'Book-Rating': 'Book_Rating'},inplace = True)
data_rating

print('Jumlah Buku berdasarkan Rating: ', len(data_rating.ISBN.unique()))
print('Jumlah Buku berdasarkan Daftar Buku: ', len(data_books.ISBN.unique()))

"""Melakukan drop kolom pada 'data_books' dikarenakan kolom tersebut tidak digunakan pada proses"""

data_books.drop(['Image-URL-S', 'Image-URL-M', 'Image-URL-L' ], axis=1, inplace=True)

"""Mengubah kolom 'Year_Publication' yang awalnya bertipe data object menjadi integer"""

data_books['Year_Publication'] = pd.to_numeric(data_books['Year_Publication'], errors='coerce')

data_books.info()

"""Menggabungkan 'data_books' dan 'data_rating' dengan menggunakan fungsi *merge*"""

data_train = data_rating.merge(data_books, left_on = 'ISBN', right_on = 'ISBN')

data_train

data_train.info()

"""Menampilkan deskripsi yang digunakan untuk memberikan informasi statistik pada kolom 'Year_Publication'"""

data_train['Year_Publication'].describe()

"""Menampilkan kolom 'Year_Publication' dengan menggunakan *barplot* dimana data tahun yang ditampilkan dari range 0-9"""

year=data_train['Year_Publication'].value_counts()[0:10]

plt.figure(figsize=(16,8))
sns.barplot(x=year.index,y=year)
plt.xticks(rotation=90)
plt.xlabel('Year')
plt.ylabel('Count')
plt.show()

"""Bila dilihat dari gambar diatas maka tahun 2002 memiliki jumlah tahun terbanyak dalam publikasi dibandingkan dari tahun yang lainnya

Selanjutnya, menampilkan 10 penulis terpopuler berasal dari kolom 'Book_Author' dimana menggunakan *barplot* untuk menampilkan gambar. Bila dilihat dari gambar dibawah ini penulis terpopuler dengan peringkat pertama yaitu 'James patterson'. Namun bila di lihat dengan seksama penulis peringkat ketiga dan keempat memiliki jumlah yang sedikit berbeda.
"""

most_author = data_train.Book_Author.value_counts().reset_index()
most_author.columns = ['Book_Author','count']

plt.figure(figsize = (16,8))
plt.title("10 Penulis Terpopuler")
sns.barplot(x = 'count', y = 'Book_Author', data = most_author.head(10), palette='icefire_r');
plt.ylabel('Book_Author')
plt.xlabel('Count')
plt.show()

"""Kemudian, menampilkan 10 publisher teratas berasal dari kolom 'Publisher' dimana menggunakan *barplot* untuk menampilkan gambar. Bila dilihat dari gambar diatas maka publisher yang berada di peringkat atas yaitu “Ballantine Books” dengan jumlah yang lebih banyak dibandingkan dengan publisher yang lain. Namun bila dilihat kembali pada peringkat sembilan dan sepuluh memiliki jumlah yang sama."""

most_publis = data_train.Publisher.value_counts().reset_index()
most_publis.columns = ['Publisher','count']

plt.figure(figsize = (16,8))
plt.title("10 Publisher teratas")
sns.barplot(x = 'count', y = 'Publisher', data = most_publis.head(12));
plt.ylabel('Publisher')
plt.xlabel('Count')
plt.show()

"""Menampilkan 'Rata-rata rating dengan buku terbanyak dibaca' dimana berasal dari kolom 'Book_Title' dan 'Book_Rating' yang menggunakan *barplot* untuk menampilkan gambar. Pada gambar dibawah terlihat bahwa buku dengan judul “A Painted House” memiliki rating terbanyak dari pengguna dibandingkan buku yang lainnya."""

data_aver = data_train.groupby('Book_Title', as_index=False)['Book_Rating'].mean()
temp = data_train.Book_Title.value_counts().reset_index()
temp.columns = ['Book_Title','count']
most_rated_by_reads = pd.merge(data_aver,temp,on='Book_Title')

most_rated_by_reads = most_rated_by_reads.sort_values('count',ascending=False)

plt.figure(figsize=(12,10))
plt.title("Rata-rata rating dengan buku terbanyak dibaca")
sns.barplot(x = 'Book_Rating', y = 'Book_Title', data = most_rated_by_reads.head(10), palette='icefire_r');

"""# **Data Preparation**

Pada tahap ini menggunakan beberapa teknik dengan penjelasan sebagai berikut:

Mengecek data pada data_books, data_rating dan gabungan dari dua data tersebut yaitu data_train dimana terdapat data null atau tidak yang menggunakan fungsi *isnull*
"""

data_books.isnull().sum()

data_rating.isnull().sum()

data_train.isnull().sum()

"""Dilakukan persiapan penghapusan data duplikat, dengan membuat variable baru dengan nama ‘data_prep’ yang berisi dataframe ‘data_train’ yang diurutkan berdasarkan ‘ISBN’"""

data_prep = data_train
data_prep.sort_values('ISBN')

"""Setelah dilakukan persiapan dilanjutkan dengan penghapusan data duplikat menggunakan fungsi *drop_duplicates*. Penghapusan data duplikat berguna bila data train dan data test ada yang sama. Bila di lihat dari gambar diatas dan di bawah jumlah rows berkurang ketika dilakukan penghapusan data duplikat"""

data_prep = data_prep.drop_duplicates('ISBN')
data_prep

"""Pada gambar diatas dilakukan proses pengkonversian data series dalam bentuk list. Pada proses menggunakan fungsi ‘tolist()’ dari library numpy. Yang kemudian menampilkan jumlah dari books_id, books_title dan books_author."""

# Mengonversi data series 'ISBN’ menjadi dalam bentuk list
books_id = data_prep['ISBN'].tolist()
 
# Mengonversi data series ‘Title’ menjadi dalam bentuk list
books_title = data_prep['Book_Title'].tolist()
 
# Mengonversi data series ‘Author’ menjadi dalam bentuk list
books_author = data_prep['Book_Author'].tolist()
 
print(len(books_id))
print(len(books_title))
print(len(books_author))

"""Tahap berikutnya, membuat dictionary yang gunanya untuk menentukan pasangan key-value dari data books_id, books_title dan books_author"""

# Membuat dictionary untuk data ‘books_id’, ‘books_title’, dan ‘books_author’
books_new = pd.DataFrame({
    'id': books_id,
    'title':books_title,
    'author': books_author
})
books_new

"""Kemudian, untuk memudahkan mengingat nama rating sehingga untuk variable 'data_rating' diubah menjadi 'df' sehingga ketika membaca dataset untuk rating menggunakan variable 'df'"""

df = data_rating
df

"""Setelah melakukan proses diatas maka masuk ke proses encoding data. Dimana pada proses ini digunakan untuk menyandikan (encode) fitur ke dalam indeks integer dimana fitur yang digunakan yaitu fitur ‘UserID’ dan ‘ISBN’.

"""

# Mengubah UserID menjadi list tanpa nilai yang sama
user_ids = df['UserID'].unique().tolist()
print('list UserID: ', user_ids)
 
# Melakukan encoding userID
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded UserID : ', user_to_user_encoded)
 
# Melakukan proses encoding angka ke UserID
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke UserID: ', user_encoded_to_user)

"""Pada output diatas menampilkan encode fitur ‘UserID’ dimana terdiri dari list UserID, encoded UserId dan encoded angka ke UserID.

Untuk proses encoding fitur ISBN sama seperti proses encoding fitur UserID yang dilanjutkan dengan memetakan UserID dan ISBN ke dataframe yang berkaitan seperti UserID ke dataframe usser dan ISBN ke dataframe book.
"""

# Mengubah ISBN menjadi list tanpa nilai yang sama
book_ids = df['ISBN'].unique().tolist()
 
# Melakukan proses encoding ISBN
book_to_book_encoded = {x: i for i, x in enumerate(book_ids)}
 
# Melakukan proses encoding angka ke ISBN
book_encoded_to_book = {i: x for i, x in enumerate(book_ids)}
 
#Selanjutnya, petakan userID dan ISBN ke dataframe yang berkaitan.
 
# Mapping userID ke dataframe user
df['user'] = df['UserID'].map(user_to_user_encoded)
 
# Mapping ISBN ke dataframe book
df['book'] = df['ISBN'].map(book_to_book_encoded)

"""Tahap terakhir yaitu melakukan pengecekan data seperti jumlah user, jumlah book dan mengubah nilai rating yang awalnya memiliki tipe data integer menjadi float."""

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)
 
# Mendapatkan jumlah book
num_book = len(book_encoded_to_book)
print(num_book)
 
# Mengubah rating menjadi nilai float
df['Book_Rating'] = df['Book_Rating'].values.astype(np.float32)
 
# Nilai minimum rating
min_rating = min(df['Book_Rating'])
 
# Nilai maksimal rating
max_rating = max(df['Book_Rating'])
 
print('Number of User: {}, Number of Resto: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_book, min_rating, max_rating
))

"""Pada output diatas ditampilkan jumlah user, jumlah book, minimum rating dan maksimal rating.

Untuk tahap ini dilakukan pengacakan dataset agar distribusi yang dilakukan menjadi random
"""

# Mengacak dataset
df = df.sample(frac=1, random_state=42)
df

"""Selanjutnya, membagi data train dan validasi dengan komposisi 90:10. Namun sebelumnya, kita perlu memetakan (mapping) data user dan book menjadi satu value terlebih dahulu. Kemudian, membuat rating dalam skala 0 sampai 1 agar mudah dalam melakukan proses training. """

# Membuat variabel x untuk mencocokkan data user dan book menjadi satu value
x = df[['user', 'book']].values
 
# Membuat variabel y untuk membuat rating dari hasil 
y = df['Book_Rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values
 
# Membagi menjadi 90% data train dan 20% data validasi
train_indices = int(0.9 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)
 
print(x, y)

"""# **Modelling**

Pada tahap ini saya menggunakan model collaborative filtering dimana menggunakan metode deep learning yang bertujuan menghasilkan rekomendasi buku.

Tahap awal yang dilakukan yaitu melakukan proses embedding terhadap data user dan book. Lalu dilanjutkan dengan operasi perkalian dot product antara embedding user dan book serta menambahkan bias untuk kedua data. Skor kecocokan di tetapkan dalam skala [0,1] dengan fungsi aktivasi sigmoid
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
class RecommenderNet(tf.keras.Model):
 
  # Insialisasi fungsi
  def __init__(self, num_users, num_book, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_book = num_book
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.book_embedding = layers.Embedding( # layer embeddings book
        num_book,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.book_bias = layers.Embedding(num_book, 1) # layer embedding book bias
 
  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    book_vector = self.book_embedding(inputs[:, 1]) # memanggil layer embedding 3
    book_bias = self.book_bias(inputs[:, 1]) # memanggil layer embedding 4
 
    dot_user_book = tf.tensordot(user_vector, book_vector, 2) 
 
    x = dot_user_book + user_bias + book_bias
    
    return tf.nn.sigmoid(x) # activation sigmoid

"""Langkah selanjutnya dengan melakukan proses compile terhadap model yang terdiri dari loss function yang menggunakan Binary Crossentropy, optimizer yang menggunakan Adam (Adaptive Moment Estimation) dan untuk metrics evaluation yaitu root mean squared error (RMSE)"""

model = RecommenderNet(num_users, num_book, 50) # inisialisasi model
 
# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""Melakukan proses training"""

# Memulai training
 
history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 8,
    epochs = 20,
    validation_data = (x_val, y_val)
)

"""Tahap akhir yaitu dengan mengambil sampel user secara acak dan definisikan variabel book_not_visited yang merupakan daftar book yang belum pernah dikunjungi oleh pengguna. Variabel book_not_visited diperoleh dengan menggunakan operator bitwise (~) pada variabel book_visited_by_user."""

book_df = books_new
 
# Mengambil sample user
user_id = df.UserID.sample(1).iloc[0]
book_visited_by_user = df[df.UserID == user_id]
 
# Operator bitwise (~), bisa diketahui di sini https://docs.python.org/3/reference/expressions.html 
book_not_visited = book_df[~book_df['id'].isin(book_visited_by_user.ISBN.values)]['id'] 
book_not_visited = list(
    set(book_not_visited)
    .intersection(set(book_to_book_encoded.keys()))
)
 
book_not_visited = [[book_to_book_encoded.get(x)] for x in book_not_visited]
user_encoder = user_to_user_encoded.get(user_id)
user_book_array = np.hstack(
    ([[user_encoder]] * len(book_not_visited), book_not_visited)
)

"""Dalam memperoleh rekomendasi buku maka menggunakan fungsi *model.predict()* dari library Keras."""

ratings = model.predict(user_book_array).flatten()
 
top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_book_ids = [
    book_encoded_to_book.get(book_not_visited[x][0]) for x in top_ratings_indices
]
 
print('Menampilkan Rekomendasi Untuk Pengguna: {}'.format(user_id))
print('===' * 9)
print('Buku dengan peringkat tinggi dari pengguna')
print('----' * 8)
 
top_book_user = (
    book_visited_by_user.sort_values(
        by = 'Book_Rating',
        ascending=False
    )
    .head(5)
    .ISBN.values
)
 
book_df_rows = book_df[book_df['id'].isin(top_book_user)]
for row in book_df_rows.itertuples():
    print(row.title, ':', row.author)
 
print('----' * 8)
print('10 Rekomendasi Buku Teratas')
print('----' * 8)
 
recommended_book = book_df[book_df['id'].isin(recommended_book_ids)]
for row in recommended_book.itertuples():
    print(row.title, ':', row.author)

"""Pada output diatas merupakan hasil rekomendasi dari model collaborative filtering dimana user dengan id 508. Kita dapat melihat bahwa terdapat dua perbandingan yaitu Buku dengan peringkat tinggi dari pengguna yaitu ‘Echoes : Maeve Binchy’ dan ‘Kissing in Manhattan : DAVID SCHICKLER’ Serta 10 Rekomendasi Buku Teratas yang salah satunya yaitu ‘The Watsons Go to Birmingham - 1963 (Yearling Newbery) : CHRISTOPHER PAUL CURTIS’.

# **Evaluation**

Pada tahap ini saya menggunakan metrik root mean squared error (RMSE) dimana metode estimasi yang mempunyai Root Mean Square Error (RMSE) lebih kecil dikatakan lebih akurat daripada metode estimasi yang mempunyai Root Mean Square Error (RMSE) lebih besar.

Pada gambar dibawah ini merupakan hasil visualisasi metrik RMSE dari proses training yang menggunakan matplotlib. Dimana menampilkan plot *root_mean_squared_error* dan *val_root_mean_squared_error*
"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""# **Kesimpulan**

Dari proses yang telah dilakukan didapatkan hasil perbandingan buku dengan peringkat tinggi dari pengguna dan 10 Rekomendasi Buku Teratas. Pada proyek ini saya menggunakan model Colaborative Filtering dimana menggunakan metrik Root Mean Square Error (RMSE). Serta untuk model ini data yang diperlukan dalam membuat rekomendasi yaitu data rating dari pengguna. Kemudian, dikarenakan data yang terlalu banyak sehingga saya menggunakan beberapa data total dari data yang ada 12.000 data Books dan 12.000 data Rating.
"""